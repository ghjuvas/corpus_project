{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сбор данных для проекта по АОЕЯ \"Корпус анекдотов\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Парсинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Парсинг ВК сообщества с анекдотами. Собираем анекдоты и мета-информацию о них"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем токен (*здесь скрыт*) и версию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = \"5.130\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wall_get_url = \"https://api.vk.com/method/wall.get\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offs = [x*100 for x in range(0, 220)]  # генерируем оффсеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_the_data = []\n",
    "for off in offs:\n",
    "    data = requests.get(\n",
    "        wall_get_url, \n",
    "        params={\n",
    "            \"owner_id\": -50384057,  # id группы, номер пишется после \"-\"\n",
    "            \"count\": 100,\n",
    "            \"v\": VERSION,\n",
    "            \"access_token\": TOKEN,\n",
    "            \"offset\": off, \n",
    "        }\n",
    "    ).json()\n",
    "    all_the_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вытаскиваем тексты из постов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for data in all_the_data:\n",
    "    texts = []\n",
    "    if len(data['response']['items']) != 0:\n",
    "        text = data['response']['items'][0]['text']\n",
    "        text_tokens = data['response']['items'][0]['text'].split()\n",
    "        if len(text_tokens) > 99: #в тексте должно быть как минимум 100 токенов\n",
    "            texts.append(i)\n",
    "            texts.append(data['response']['items'][0]['id'])\n",
    "            texts.append(f\"https://vk.com/wall{data['response']['items'][0]['from_id']}_{data['response']['items'][0]['id']}\")\n",
    "            texts.append(text)\n",
    "            texts.append(text_tokens)\n",
    "            text_sntnss = text.split('.')\n",
    "            texts.append(text_sntnss)\n",
    "            i += 1\n",
    "    texts_data.append(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_data2 = [text_data for text_data in texts_data if text_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаем датафрейм с помощью ``pandas``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(texts_data2, columns = ['id', 'vk_id', 'link', 'text', 'tokens', 'sentences'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Записываем в два формата: ``.xlsx`` и ``.csv``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('vk_texts.xlsx', engine='xlsxwriter')\n",
    "df.to_excel(writer, 'Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('csv_vk_texts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Морфология"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Парсим скачанные тексты с помощью NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stanza\n",
    "import stanza\n",
    "stanza.download('ru')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С текстами были некоторые проблемы, которые мы решили препроцессингом: тексты начинались с латинских симоволов, которые были заменены на кириллические:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "rows = ''\n",
    "nltk.download('punkt')\n",
    "from string import punctuation\n",
    "my_parse = {}\n",
    "with open ('csv_vk_texts.csv') as f:\n",
    "  reader = csv.reader(f, delimiter = ',')\n",
    "  for row in reader:\n",
    "    rows += row[4] + '#__#'\n",
    "  alph = {'a':'а','b':'в', 'c':'с', 'd':'о', 'e':'е', 'h':'н', 'k':'к', 'm':'м', 'o':'о', 'p':'р', 't':'т', 'u' :'и', 'x':'х', 'y':'у', 'A':'А', 'B':'В', 'C':'С', 'E':'Е', 'H':'Н', 'K':'К', 'M':'М', 'N':'Н', 'O':'О', 'P':'Р', 'T':'Т', 'X':'Х', 'Y':'У'}\n",
    "  for key in alph:\n",
    "    rows = rows.replace(key, alph[key]) \n",
    "  rows_new = rows.split('#__#')\n",
    "  rows_new.remove(rows_new[0])\n",
    "  for idx, row in enumerate(rows_new):\n",
    "    text_sents = sent_tokenize(row)  # делим на предложения, чтобы знать их индексы в тексте\n",
    "    #words_new = re.sub('T', 'т', row[3])\n",
    "    #print(words)\n",
    "    sent_idx = {}\n",
    "    count = 0\n",
    "    for sent in text_sents:\n",
    "      sent_idx[count] = sent\n",
    "      count += 1\n",
    "    my_parse[idx] = sent_idx\n",
    "print(my_parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(lang='ru', processors='tokenize,lemma,pos')\n",
    "def stanza_parse(sent):\n",
    "  doc = nlp(sent)\n",
    "  new_list = []\n",
    "  for sent in doc.sentences:\n",
    "    for word in sent.words:\n",
    "      if word.upos != 'PUNCT':\n",
    "        new_list.append((word.text, word.lemma, word.upos))\n",
    "  return new_list\n",
    "#print(stanza_parse(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_dict = {}\n",
    "\n",
    "for text_id, sents in my_parse.items():\n",
    "  text_dict = {}\n",
    "  for idx, elem in sents.items():\n",
    "    text_dict[idx] = stanza_parse(elem)\n",
    "  parse_dict[text_id] = text_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пишем файлы: первый для предложений и их индексов. а другой для словоформ и тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sents_id.csv', 'w') as sents_file:\n",
    "    writer = csv.writer(sents_file)\n",
    "    writer.writerow(['id_text', 'id_sent', 'sent'])\n",
    "\n",
    "    for key, value in my_parse.items():\n",
    "        id_text = key\n",
    "        for x, y in value.items():\n",
    "            id_sent = x\n",
    "            sent = y\n",
    "            writer.writerow([id_text, id_sent, sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('word_pos_lemma_stanza.csv', 'w') as file:\n",
    "  writer = csv.writer(file)\n",
    "  writer.writerow(['id_text', 'id_sent','word', 'lemma', 'pos'])\n",
    "\n",
    "  for key, value in parse_dict.items():\n",
    "    id_text = key\n",
    "    #print(value)\n",
    "    for x, y in value.items():\n",
    "      id_sent = x\n",
    "      for listt in y:\n",
    "        word = listt[0].lower()\n",
    "        lemma = listt[1].lower()\n",
    "        pos = listt[2]\n",
    "        writer.writerow([id_text, id_sent, word, lemma, pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## База данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаём базу данных и складываем туда обработанные данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('anecdotes.db')\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('''\n",
    "CREATE TABLE IF NOT EXISTS texts_meta\n",
    "(id INTEGER PRIMARY KEY,\n",
    "vk_id text,\n",
    "text text,\n",
    "link text)\n",
    "''')\n",
    "\n",
    "cur.execute('''\n",
    "CREATE TABLE IF NOT EXISTS morph_parse\n",
    "(id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "text_id int,\n",
    "sent_id int,\n",
    "token text,\n",
    "lemma text,\n",
    "POS text)\n",
    "''')\n",
    "\n",
    "cur.execute('''\n",
    "CREATE TABLE IF NOT EXISTS sents\n",
    "(id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "text_id int,\n",
    "sent_id int,\n",
    "sent text)\n",
    "''')\n",
    "\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Записываем данные в базу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('csv_vk_texts.csv') as texts_csv:\n",
    "    texts_reader = csv.DictReader(texts_csv, delimiter=',')\n",
    "    \n",
    "    for text_row in texts_reader:\n",
    "        text_id = text_row['id']\n",
    "        vk_id = text_row['vk_id']\n",
    "        link = text_row['link']\n",
    "        text = text_row['text']\n",
    "\n",
    "        cur.execute('''INSERT INTO texts_meta (id, vk_id, text, link)\n",
    "        VALUES (?, ?, ?, ?)''',\n",
    "        (int(text_id), vk_id, text, link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('word_pos_lemma_stanza.csv') as morph_csv:\n",
    "    morph_reader = csv.DictReader(morph_csv, delimiter=',')\n",
    "\n",
    "    for morph_row in morph_reader:\n",
    "        text_id = morph_row['id_text']\n",
    "        sent_id = morph_row['id_sent']\n",
    "        token = morph_row['word']\n",
    "        lemma = morph_row['lemma']\n",
    "        pos = morph_row['pos']\n",
    "\n",
    "        cur.execute('''INSERT INTO morph_parse (text_id, sent_id, token, lemma, POS)\n",
    "        VALUES (?, ?, ?, ?, ?)''',\n",
    "        (int(text_id), int(sent_id), token, lemma, pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('sents_id.csv') as sents_csv:\n",
    "    sents_reader = csv.DictReader(sents_csv, delimiter=',')\n",
    "\n",
    "    for sent_row in sents_reader:\n",
    "        sent_id = sent_row['id_sent']\n",
    "        text_id = sent_row['id_text']\n",
    "        sent = sent_row['sent']\n",
    "\n",
    "        cur.execute('''INSERT INTO sents (text_id, sent_id, sent)\n",
    "        VALUES (?, ?, ?)''',\n",
    "        (int(text_id), int(sent_id), sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.commit()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
